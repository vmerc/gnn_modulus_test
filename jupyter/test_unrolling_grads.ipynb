{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbd9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hydra\n",
    "from hydra.utils import to_absolute_path\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import dgl\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "#project_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', ''))\n",
    "#sys.path.append(project_path)\n",
    "project_path = os.path.abspath(os.path.join(os.getcwd(), '..', ''))\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from python.create_dgl_dataset import TelemacDataset\n",
    "from modulus.distributed.manager import DistributedManager\n",
    "from modulus.launch.logging import (\n",
    "    PythonLogger,\n",
    "    RankZeroLoggingWrapper,\n",
    "    initialize_wandb,\n",
    ")\n",
    "from modulus.launch.utils import load_checkpoint, save_checkpoint\n",
    "from python.CustomMeshGraphNet import MeshGraphNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633ebaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac261e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:57:36 - checkpoint - INFO] \u001b[92mLoaded model state dictionary /work/m24046/m24046mrcr/work/m24046/m24046mrcr/new_tests_Group3/config1/MeshGraphNet.0.1310.mdlus to device cuda\u001b[0m\n",
      "[16:57:36 - checkpoint - INFO] \u001b[92mLoaded checkpoint file /work/m24046/m24046mrcr/work/m24046/m24046mrcr/new_tests_Group3/config1/checkpoint.0.1310.pt to device cuda\u001b[0m\n",
      "[16:57:36 - checkpoint - INFO] \u001b[92mLoaded optimizer state dictionary\u001b[0m\n",
      "[16:57:36 - checkpoint - INFO] \u001b[92mLoaded scheduler state dictionary\u001b[0m\n",
      "[16:57:36 - checkpoint - INFO] \u001b[92mLoaded grad scaler state dictionary\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure that DGL and other dependencies are installed\n",
    "# !pip install dgl\n",
    "\n",
    "# Define your collate function\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of sequences\n",
    "    # Each sequence is a list of graphs (of length sequence_length)\n",
    "    # We want to batch the graphs at each time step across sequences\n",
    "\n",
    "    sequence_length = len(batch[0])  # Assuming all sequences have the same length\n",
    "\n",
    "    batched_graphs = []\n",
    "    for t in range(sequence_length):\n",
    "        graphs_at_t = [sequence[t] for sequence in batch]\n",
    "        batched_graph = dgl.batch(graphs_at_t)\n",
    "        batched_graphs.append(batched_graph)\n",
    "\n",
    "    return batched_graphs\n",
    "\n",
    "\n",
    "\n",
    "# Define the trainer class\n",
    "class MGNTrainer:\n",
    "    def __init__(self):\n",
    "        self.sequence_length = 10  # Adjust as needed\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # Instantiate the dataset and dataloader\n",
    "        dataset = TelemacDataset(\n",
    "            name=\"telemac_train\",\n",
    "            data_dir=\"/work/m24046/m24046mrcr/results_data_30min/Multimesh_2_32_True.bin\",\n",
    "            dynamic_data_files=['/work/m24046/m24046mrcr/results_data_30min/Group_3_peak_1800_Group_3_peak_1800_0_0-80.pkl'],\n",
    "            split=\"train\",\n",
    "            ckpt_path=\"/work/m24046/m24046mrcr//work/m24046/m24046mrcr/new_tests_Group3/config1\",\n",
    "            normalize=True,\n",
    "            sequence_length=self.sequence_length,\n",
    "        )\n",
    "        self.dataloader = GraphDataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            pin_memory=True,\n",
    "            use_ddp=False,\n",
    "            num_workers=1,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "        # Instantiate the model\n",
    "        # Instantiate the model\n",
    "        self.model = MeshGraphNet(\n",
    "            9,\n",
    "            3,\n",
    "            3,\n",
    "            processor_size=10,\n",
    "            hidden_dim_processor=64,\n",
    "            hidden_dim_node_encoder=64,\n",
    "            hidden_dim_edge_encoder=64,\n",
    "            hidden_dim_node_decoder=64,\n",
    "            do_concat_trick=True,\n",
    "            num_processor_checkpoint_segments=1,\n",
    "        )\n",
    "        if 0 :\n",
    "            self.model = torch.jit.script(self.model).to(self.device)\n",
    "        else:\n",
    "            self.model = self.model.to(self.device)\n",
    "            \n",
    "        self.model.train()\n",
    "        \n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.optimizer = None\n",
    "        try:\n",
    "            if True:\n",
    "                from apex.optimizers import FusedAdam\n",
    "\n",
    "                self.optimizer = FusedAdam(self.model.parameters(), lr=0.0001)\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"NVIDIA Apex (https://github.com/nvidia/apex) is not installed, \"\n",
    "                \"FusedAdam optimizer will not be used.\"\n",
    "            )\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, lr_lambda=lambda epoch: 0.999999**epoch\n",
    "        )\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "            \n",
    "        self.epoch_init = load_checkpoint(\n",
    "            to_absolute_path(\"/work/m24046/m24046mrcr//work/m24046/m24046mrcr/new_tests_Group3/config1\"),\n",
    "            models=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            scaler=self.scaler,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        # Get node statistics from the dataset\n",
    "        node_stats = dataset.node_stats\n",
    "        device = torch.device('cuda')  # Use 'cuda' if GPU is available\n",
    "\n",
    "        self.h_u_v_i_0_mean = torch.tensor([\n",
    "            node_stats['h'],\n",
    "            node_stats['u'],\n",
    "            node_stats['v']\n",
    "        ], dtype=torch.float32).to(device)\n",
    "\n",
    "        self.h_u_v_i_0_std = torch.tensor([\n",
    "            node_stats['h_std'],\n",
    "            node_stats['u_std'],\n",
    "            node_stats['v_std']\n",
    "        ], dtype=torch.float32).to(device)\n",
    "\n",
    "        self.delta_h_u_v_i_diff_mean = torch.tensor([\n",
    "            node_stats['delta_h'],\n",
    "            node_stats['delta_u'],\n",
    "            node_stats['delta_v']\n",
    "        ], dtype=torch.float32).to(device)\n",
    "\n",
    "        self.delta_h_u_v_i_diff_std = torch.tensor([\n",
    "            node_stats['delta_h_std'],\n",
    "            node_stats['delta_u_std'],\n",
    "            node_stats['delta_v_std']\n",
    "        ], dtype=torch.float32).to(device)\n",
    "\n",
    "    def _denormalize_data(self, tensor, mean, std):\n",
    "        return tensor * std + mean\n",
    "\n",
    "    def _normalize_data(self, tensor, mean, std):\n",
    "        return (tensor - mean) / std\n",
    "\n",
    "    def modify_graph(self, graph_0, pred_0, graph_1):\n",
    "        # Clone graph_0 to avoid modifying the original\n",
    "        predicted_modified_graph = graph_0.clone()\n",
    "\n",
    "        pred_0_detached = pred_0\n",
    "\n",
    "        # Update node features in predicted_modified_graph\n",
    "        node_features_mod = predicted_modified_graph.ndata['x'].clone()\n",
    "        one_hot_vectors = node_features_mod[:, :4]\n",
    "\n",
    "        # Nodes with one-hot [0, 0, 1, 0]: replace x values with those from graph_1\n",
    "        mask_replace = (one_hot_vectors == torch.tensor([0, 0, 1, 0], device=one_hot_vectors.device)).all(dim=1)\n",
    "        node_features_mod[mask_replace] = graph_1.ndata['x'][mask_replace]\n",
    "        # Nodes with one-hot [0, 1, 0, 0]: replace 7th feature with that from graph_1\n",
    "        mask_7th = (one_hot_vectors == torch.tensor([0, 1, 0, 0], device=one_hot_vectors.device)).all(dim=1)\n",
    "        node_features_mod[mask_7th, 6] = graph_1.ndata['x'][mask_7th, 6]\n",
    "        # For other nodes, update 'h', 'u', 'v' features with pred_0\n",
    "        mask_other = ~(mask_replace | mask_7th)\n",
    "\n",
    "        # Denormalize current 'h', 'u', 'v' features\n",
    "        node_features_to_update = node_features_mod[mask_other][:, 6:9]\n",
    "        unnormalized_current = self._denormalize_data(\n",
    "            node_features_to_update,\n",
    "            self.h_u_v_i_0_mean,\n",
    "            self.h_u_v_i_0_std\n",
    "        )\n",
    "\n",
    "        # Denormalize predicted increments\n",
    "        unnormalized_increment = self._denormalize_data(\n",
    "            pred_0_detached[mask_other],\n",
    "            self.delta_h_u_v_i_diff_mean,\n",
    "            self.delta_h_u_v_i_diff_std\n",
    "        )\n",
    "\n",
    "        # Update features\n",
    "        unnormalized_updated = unnormalized_current + unnormalized_increment\n",
    "\n",
    "        # Normalize updated features\n",
    "        normalized_updated = self._normalize_data(\n",
    "            unnormalized_updated,\n",
    "            self.h_u_v_i_0_mean,\n",
    "            self.h_u_v_i_0_std\n",
    "        )\n",
    "        \n",
    "        # Update the node features in predicted_modified_graph\n",
    "        node_features_mod[mask_other][:, 6:9] = normalized_updated\n",
    "\n",
    "        predicted_modified_graph.ndata['x'] = node_features_mod\n",
    "\n",
    "        return predicted_modified_graph\n",
    "\n",
    "    def compute_loss(self, pred, target, graph):\n",
    "        # Masks and loss computation as before\n",
    "        one_hot_vectors = graph.ndata['x'][:, :4]\n",
    "        mask_exclude = (one_hot_vectors == torch.tensor([0, 0, 1, 0], device=self.device)).all(dim=1)\n",
    "        mask_include = ~mask_exclude\n",
    "        mask_specific = (one_hot_vectors == torch.tensor([0, 1, 0, 0], device=self.device)).all(dim=1)\n",
    "        mask_specific = mask_specific & mask_include\n",
    "        mask_other = mask_include & ~mask_specific\n",
    "\n",
    "        # Nodes with one-hot [0, 1, 0, 0]: predict only y[:,1:3]\n",
    "        pred_specific = pred[mask_specific][:, 1:3]\n",
    "        target_specific = target[mask_specific][:, 1:3]\n",
    "\n",
    "        # Other nodes: predict all features\n",
    "        pred_other = pred[mask_other]\n",
    "        target_other = target[mask_other]\n",
    "\n",
    "        # Compute losses\n",
    "        loss_specific = self.criterion(pred_specific, target_specific)\n",
    "        loss_other = self.criterion(pred_other, target_other)\n",
    "\n",
    "        # Combine losses\n",
    "        total_nodes = mask_include.sum()\n",
    "        weight_specific = mask_specific.sum().float() / total_nodes\n",
    "        weight_other = mask_other.sum().float() / total_nodes\n",
    "\n",
    "        loss = weight_specific * loss_specific + weight_other * loss_other\n",
    "        return loss\n",
    "\n",
    "    def forward(self, batch):\n",
    "        losses = []\n",
    "        preds = []\n",
    "        num_steps = self.sequence_length\n",
    "\n",
    "        # Initialize the first graph\n",
    "        graph = batch[0].to(self.device)\n",
    "        node_features = graph.ndata[\"x\"]\n",
    "        edge_features = graph.edata.get(\"x\", None)\n",
    "        target = graph.ndata[\"y\"]\n",
    "        with autocast(enabled=True):\n",
    "            pred = self.model(node_features, edge_features, graph)\n",
    "            loss = self.compute_loss(pred, target, graph)\n",
    "        losses.append(loss)\n",
    "        preds.append(pred)\n",
    "\n",
    "        # Loop through the sequence\n",
    "        for t in range(1, num_steps):\n",
    "            next_graph = batch[t].to(self.device)\n",
    "            # Modify the current graph using the previous prediction\n",
    "            graph = self.modify_graph(graph, preds[-1], next_graph)\n",
    "            node_features = graph.ndata[\"x\"]\n",
    "            edge_features = graph.edata.get(\"x\", None)\n",
    "            target = next_graph.ndata[\"y\"]\n",
    "            with autocast(enabled=True):\n",
    "                pred = self.model(node_features, edge_features, graph)\n",
    "                loss = self.compute_loss(pred, target, graph)\n",
    "            losses.append(loss)\n",
    "            preds.append(pred)\n",
    "        print(losses)\n",
    "        #losses[0] = 0.0\n",
    "        # Combine losses\n",
    "        total_loss = sum(losses)\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Enable gradient tracking for initial node features\n",
    "        #batch[0].ndata['x'].requires_grad_(True)\n",
    "\n",
    "        # Forward pass\n",
    "        total_loss = self.forward(batch)\n",
    "\n",
    "        # Backward pass\n",
    "        self.scaler.scale(total_loss)\n",
    "        self.scaler.scale(total_loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        # Check gradients of initial node features\n",
    "        #initial_node_features_grad = batch[0].ndata['x'].grad\n",
    "        #if initial_node_features_grad is not None:\n",
    "        #    print(\"Gradient w.r.t initial node features exists.\")\n",
    "        #    print(\"Gradient norm:\", initial_node_features_grad.norm().item())\n",
    "        #else:\n",
    "        #    print(\"No gradient w.r.t initial node features.\")\n",
    "\n",
    "        # Check if gradients are flowing through the model parameters\n",
    "        #for name, param in self.model.named_parameters():\n",
    "        #    if param.grad is not None:\n",
    "        #        print(f\"Gradient for {name}: {param.grad.norm().item()}\")\n",
    "        #    else:\n",
    "        #        print(f\"No gradient for {name}\")\n",
    "\n",
    "        \n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "# Instantiate the trainer\n",
    "trainer = MGNTrainer()\n",
    "\n",
    "# Run the training step\n",
    "#for batch in trainer.dataloader:\n",
    "#    loss = trainer.train(batch)\n",
    "#    print(f\"Training loss: {loss}\")\n",
    "#    break  # Only run one batch for the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3014c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.2707, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.4414, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2742, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.3573, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2783, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.4228, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2855, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2077, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2994, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "batch loss: 2.8932418823242188\n",
      "[tensor(0.1902, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2730, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.3146, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2531, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.3145, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.3769, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.4750, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "batch loss: 2.503706693649292\n",
      "[tensor(0.3613, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.3272, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2182, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2575, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2451, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.2219, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.4372, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.6925, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "batch loss: 3.2208664417266846\n",
      "[tensor(1.1308, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.3893, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.9513, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7797, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.6708, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.6448, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.7141, device='cuda:0', grad_fn=<AddBackward0>), tensor(0.8485, device='cuda:0', grad_fn=<AddBackward0>), tensor(1.3714, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "batch loss: 8.98119068145752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for epoch in range(100):\n",
    "    total_loss = 0 \n",
    "    for batch in trainer.dataloader:\n",
    "        loss = trainer.train(batch)\n",
    "        total_loss += loss.item()\n",
    "        print(f\"batch loss: {loss.item()}\")\n",
    "                \n",
    "    total_loss = total_loss / len(trainer.dataloader)\n",
    "    print(f\"Training loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9dd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
