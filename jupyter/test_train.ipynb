{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe3829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hydra\n",
    "from hydra.utils import to_absolute_path\n",
    "import torch\n",
    "import sys\n",
    "import os \n",
    "import numpy as np \n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "project_path = os.path.abspath(os.path.join(os.path.abspath(''), '..', ''))\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from python.create_dgl_dataset import TelemacDataset\n",
    "from modulus.distributed.manager import DistributedManager\n",
    "from modulus.launch.logging import (\n",
    "    PythonLogger,\n",
    "    RankZeroLoggingWrapper,\n",
    "    initialize_wandb,\n",
    ")\n",
    "from modulus.launch.utils import load_checkpoint, save_checkpoint\n",
    "from python.CustomMeshGraphNet import MeshGraphNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6ce5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGNTrainer:\n",
    "    def __init__(self, cfg: DictConfig, rank_zero_logger: RankZeroLoggingWrapper):\n",
    "        assert DistributedManager.is_initialized()\n",
    "        self.dist = DistributedManager()\n",
    "\n",
    "        self.amp = cfg.amp\n",
    "\n",
    "        # instantiate dataset\n",
    "        dataset = TelemacDataset(\n",
    "            name=\"telemac_train\",\n",
    "            data_dir=to_absolute_path(cfg.data_dir),\n",
    "            split=\"train\",\n",
    "            num_samples=cfg.num_training_samples,\n",
    "            num_steps=cfg.num_training_time_steps\n",
    "        )\n",
    "\n",
    "        # instantiate dataloader\n",
    "        self.dataloader = GraphDataLoader(\n",
    "            dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=True,\n",
    "            use_ddp=self.dist.world_size > 1,\n",
    "            num_workers=cfg.num_dataloader_workers,\n",
    "        )\n",
    "\n",
    "        # instantiate the model\n",
    "        self.model = MeshGraphNet(\n",
    "            cfg.num_input_features,\n",
    "            cfg.num_edge_features,\n",
    "            cfg.num_output_features,\n",
    "            processor_size=4,\n",
    "            hidden_dim_processor=64,\n",
    "            hidden_dim_node_encoder=64,\n",
    "            hidden_dim_edge_encoder=64,\n",
    "            hidden_dim_node_decoder=64,\n",
    "            do_concat_trick=cfg.do_concat_trick,\n",
    "            num_processor_checkpoint_segments=cfg.num_processor_checkpoint_segments,\n",
    "        )\n",
    "        if cfg.jit:\n",
    "            if not self.model.meta.jit:\n",
    "                raise ValueError(\"MeshGraphNet is not yet JIT-compatible.\")\n",
    "            self.model = torch.jit.script(self.model).to(self.dist.device)\n",
    "        else:\n",
    "            self.model = self.model.to(self.dist.device)\n",
    "        \n",
    "\n",
    "        # distributed data parallel for multi-node training\n",
    "        if self.dist.world_size > 1:\n",
    "            self.model = DistributedDataParallel(\n",
    "                self.model,\n",
    "                device_ids=[self.dist.local_rank],\n",
    "                output_device=self.dist.device,\n",
    "                broadcast_buffers=self.dist.broadcast_buffers,\n",
    "                find_unused_parameters=self.dist.find_unused_parameters,\n",
    "            )\n",
    "\n",
    "        # enable train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # instantiate loss, optimizer, and scheduler\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.optimizer = None\n",
    "        try:\n",
    "            if cfg.use_apex:\n",
    "                from apex.optimizers import FusedAdam\n",
    "\n",
    "                self.optimizer = FusedAdam(self.model.parameters(), lr=cfg.lr)\n",
    "        except ImportError:\n",
    "            rank_zero_logger.warning(\n",
    "                \"NVIDIA Apex (https://github.com/nvidia/apex) is not installed, \"\n",
    "                \"FusedAdam optimizer will not be used.\"\n",
    "            )\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=cfg.lr)\n",
    "        rank_zero_logger.info(f\"Using {self.optimizer.__class__.__name__} optimizer\")\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.optimizer, lr_lambda=lambda epoch: cfg.lr_decay_rate**epoch\n",
    "        )\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "        # load checkpoint\n",
    "        if self.dist.world_size > 1:\n",
    "            torch.distributed.barrier()\n",
    "            \n",
    "        self.epoch_init = load_checkpoint(\n",
    "            to_absolute_path(cfg.ckpt_path),\n",
    "            models=self.model,\n",
    "            optimizer=self.optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "            scaler=self.scaler,\n",
    "            device=self.dist.device,\n",
    "        )\n",
    "\n",
    "    def train(self, graph):\n",
    "        graph = graph.to(self.dist.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward(graph)\n",
    "        self.backward(loss)\n",
    "        self.scheduler.step()\n",
    "        return loss\n",
    "\n",
    "    #def forward(self, graph):\n",
    "    #    # forward pass\n",
    "    #    with autocast(enabled=self.amp):\n",
    "    #        pred = self.model(graph.ndata[\"x\"], graph.edata[\"x\"], graph)\n",
    "    #        #print(np.sum((graph.ndata['x'][:, 1] == 1).cpu().detach().numpy()))\n",
    "    #        #print(np.sum((graph.ndata['x'][:, 0] == 1).cpu().detach().numpy()))\n",
    "    #        #print(np.sum(((graph.ndata['x'][:, 0] == 1)|(graph.ndata['x'][:, 1] == 1)).cpu().detach().numpy()))\n",
    "    #        mask_full = (graph.ndata['x'][:, 0] == 1)  # Mask for [1,0,0,0] nodes\n",
    "    #        mask_partial = (graph.ndata['x'][:, 1] == 1)\n",
    "    #        loss = self.criterion(pred[mask_full], graph.ndata[\"y\"][mask_full])\n",
    "    #        loss2 = self.criterion(pred[mask_partial][:,-2:], graph.ndata[\"y\"][mask_partial][:,-2:])\n",
    "    #        print('loss {}'.format(loss.cpu().detach()))\n",
    "    #        print('loss2 {}'.format(loss2.cpu().detach()))\n",
    "    #        return loss\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        # Forward pass\n",
    "        with autocast(enabled=self.amp):\n",
    "            pred = self.model(graph.ndata[\"x\"], graph.edata[\"x\"], graph)\n",
    "            #print(\"pred shape {}\".format(pred.shape))\n",
    "    \n",
    "            # Extracting the target labels\n",
    "            target = graph.ndata['y']\n",
    "            #print(\"target shape {}\".format(target.shape))\n",
    "    \n",
    "            # Create masks for different node types\n",
    "            mask_full = (graph.ndata['x'][:, 0] == 1)  # Mask for [1,0,0,0] nodes\n",
    "            mask_partial = (graph.ndata['x'][:, 1] == 1)  # Mask for [0,1,0,0] nodes\n",
    "            \n",
    "            coeff_full = mask_full.shape[0]/(mask_full.shape[0]+mask_partial.shape[0])\n",
    "            \n",
    "            coeff_partial = mask_partial.shape[0]/(mask_full.shape[0]+mask_partial.shape[0])\n",
    "            \n",
    "            #print('mask full shape {}'.format(mask_full.shape))\n",
    "            #print('mask partial shape {}'.format(mask_partial.shape))\n",
    "    \n",
    "            # Initialize loss to zero, ensure it has gradient support if no nodes are selected\n",
    "            loss = torch.tensor(0.0, device=self.dist.device, requires_grad=True)\n",
    "    \n",
    "            # Compute loss for fully considered nodes\n",
    "            if torch.any(mask_full):\n",
    "                pred_full = pred[mask_full]\n",
    "                target_full = target[mask_full]\n",
    "                \n",
    "                #print('pred full shape {}'.format(pred_full.shape))\n",
    "                #print('target full shape {}'.format(target_full.shape))\n",
    "                \n",
    "                loss_full = self.criterion(pred_full, target_full)\n",
    "                #print(loss_full)\n",
    "                loss = loss + coeff_full*loss_full\n",
    "    \n",
    "            # Compute loss for partially considered nodes (last two dimensions)\n",
    "            if torch.any(mask_partial):\n",
    "                pred_partial = pred[mask_partial][:, -2:]  # Select only the last two dimensions\n",
    "                target_partial = target[mask_partial][:, -2:]  # Corresponding targets\n",
    "                \n",
    "                #print('pred partial shape {}'.format(pred_partial.shape))\n",
    "                #print('target partial shape {}'.format(target_partial.shape))\n",
    "                \n",
    "                loss_partial = self.criterion(pred_partial, target_partial)\n",
    "                #print(loss_partial)\n",
    "                loss = loss + coeff_partial*loss_partial\n",
    "    \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def backward(self, loss):\n",
    "        # backward pass\n",
    "        if self.amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a2052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8191/1157251147.py:42: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../bin/conf\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir: ./data/toy_one_traj.bin\n",
      "batch_size: 10\n",
      "epochs: 1000\n",
      "num_training_samples: 1\n",
      "num_training_time_steps: 720\n",
      "lr: 0.0005\n",
      "lr_decay_rate: 0.9999991\n",
      "num_input_features: 9\n",
      "num_output_features: 3\n",
      "num_edge_features: 3\n",
      "use_apex: true\n",
      "amp: false\n",
      "jit: false\n",
      "num_dataloader_workers: 4\n",
      "do_concat_trick: true\n",
      "num_processor_checkpoint_segments: 0\n",
      "recompute_activation: false\n",
      "ckpt_path: ./data/checkpoints\n",
      "\n",
      "{'h': tensor([0.0953]), 'u': tensor([-0.0004]), 'v': tensor([0.1140]), 'strickler': tensor([16.2529]), 'z': tensor([-0.5487]), 'delta_h': tensor([0.0002]), 'delta_u': tensor([-7.2438e-07]), 'delta_v': tensor([0.0002]), 'h_std': tensor([0.3001]), 'u_std': tensor([0.0503]), 'v_std': tensor([0.3657]), 'strickler_std': tensor([7.2369]), 'z_std': tensor([0.8599]), 'delta_h_std': tensor([0.0034]), 'delta_u_std': tensor([0.0113]), 'delta_v_std': tensor([0.0140])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:47:12 - main - INFO] \u001b[94mUsing FusedAdam optimizer\u001b[0m\n",
      "[15:47:12 - checkpoint - INFO] \u001b[92mLoaded model state dictionary /users/daml/vmercier/gnn_modulus_project/data/checkpoints/MeshGraphNet.0.980.mdlus to device cuda:0\u001b[0m\n",
      "[15:47:12 - checkpoint - INFO] \u001b[92mLoaded checkpoint file /users/daml/vmercier/gnn_modulus_project/data/checkpoints/checkpoint.0.980.pt to device cuda:0\u001b[0m\n",
      "[15:47:12 - checkpoint - INFO] \u001b[92mLoaded optimizer state dictionary\u001b[0m\n",
      "[15:47:12 - checkpoint - INFO] \u001b[92mLoaded scheduler state dictionary\u001b[0m\n",
      "[15:47:12 - checkpoint - INFO] \u001b[92mLoaded grad scaler state dictionary\u001b[0m\n",
      "[15:47:12 - main - INFO] \u001b[94mTraining started...\u001b[0m\n",
      "[15:47:46 - main - INFO] \u001b[94mepoch: 0, loss: 1.057e-02, time per epoch: 3.387e+01\u001b[0m\n",
      "[15:47:46 - checkpoint - INFO] \u001b[92mSaved model state dictionary: /users/daml/vmercier/gnn_modulus_project/data/checkpoints/MeshGraphNet.0.0.mdlus\u001b[0m\n",
      "[15:47:46 - checkpoint - INFO] \u001b[92mSaved training checkpoint: /users/daml/vmercier/gnn_modulus_project/data/checkpoints/checkpoint.0.0.pt\u001b[0m\n",
      "[15:47:46 - main - INFO] \u001b[94mSaved model on rank 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "def train_model(cfg: DictConfig):\n",
    "    # Initialize the distributed manager\n",
    "    DistributedManager.initialize()\n",
    "    dist = DistributedManager()\n",
    "\n",
    "    logger = PythonLogger(\"main\")\n",
    "    rank_zero_logger = RankZeroLoggingWrapper(logger, dist)\n",
    "    rank_zero_logger.file_logging()\n",
    "\n",
    "    trainer = MGNTrainer(cfg, rank_zero_logger)\n",
    "    start = time.time()\n",
    "    rank_zero_logger.info(\"Training started...\")\n",
    "    for epoch in range(100):\n",
    "        for graph in trainer.dataloader:\n",
    "            loss = trainer.train(graph)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            rank_zero_logger.info(\n",
    "                f\"epoch: {epoch}, loss: {loss:.3e}, time per epoch: {(time.time() - start):.3e}\"\n",
    "            )\n",
    "        # Save checkpoint\n",
    "        if dist.world_size > 1:\n",
    "            torch.distributed.barrier()\n",
    "        if dist.rank == 0 and epoch % 20 == 0:\n",
    "            save_checkpoint(\n",
    "                to_absolute_path(cfg.ckpt_path),\n",
    "                models=trainer.model,\n",
    "                optimizer=trainer.optimizer,\n",
    "                scheduler=trainer.scheduler,\n",
    "                scaler=trainer.scaler,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            logger.info(f\"Saved model on rank {dist.rank}\")\n",
    "        start = time.time()\n",
    "    rank_zero_logger.info(\"Training completed!\")\n",
    "\n",
    "# Initialize Hydra and set the configuration directory\n",
    "with initialize(config_path=\"../bin/conf\"):\n",
    "    # Compose the configuration using the config name\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    \n",
    "    # Display the configuration (optional)\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # Now call the training function with the composed config\n",
    "    train_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf708e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
